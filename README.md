# Projet Data Engineering - E-commerce de CosmÃ©tiques Biologiques

Ce projet vise Ã  amÃ©liorer le retour sur investissement (ROI) des campagnes marketing d'une entreprise de cosmÃ©tiques biologiques en fusionnant des donnÃ©es provenant de multiples sources (logs web, CRM, plateformes publicitaires) pour obtenir une vision globale des performances marketing.

## ğŸŒŸ FonctionnalitÃ©s

- Ingestion multi-source de donnÃ©es (Web, CRM, PublicitÃ©s, Influenceurs)
- Pipelines ETL pour nettoyer, transformer et enrichir les donnÃ©es
- Orchestration automatisÃ©e des flux de donnÃ©es
- Tableaux de bord interactifs pour l'analyse des performances marketing
- Environnement technique conteneurisÃ© avec Docker

## ğŸ—ï¸ Architecture du systÃ¨me

Ce projet utilise une architecture moderne de data engineering basÃ©e sur le modÃ¨le Lambda (traitement batch et temps rÃ©el) avec les composants suivants:

### ğŸ“Š Flux de DonnÃ©es

#### â­ Streaming Data Path (Logs Web)

- **Flux** : Logs Web â†’ Kafka â†’ Spark Streaming â†’ Data Lake  
- **Objectif** : OptimisÃ© pour le traitement des Ã©vÃ©nements en temps rÃ©el  
- **Avantages** : Exploite les capacitÃ©s de streaming de Kafka pour gÃ©rer des Ã©vÃ©nements Ã  haut dÃ©bit  

#### â­ Batch Data Path (CRM & PublicitÃ©s)

- **Flux** : DonnÃ©es CRM/PublicitÃ©s â†’ Spark Batch â†’ Data Lake  
- **Objectif** : Ingestion directe dans Spark pour un traitement en lot  
- **Avantages** : Ã‰vite l'utilisation inutile de Kafka pour des chargements de donnÃ©es pÃ©riodiques  

---

### ğŸ› ï¸ Composants de l'architecture

#### ğŸ”¹ Sources de donnÃ©es

- **Logs Web** : Ã‰vÃ©nements de comportement utilisateur, pages vues, conversions  
- **DonnÃ©es CRM** : Profils clients, historique des commandes  
- **DonnÃ©es Publicitaires** : Performance des campagnes, dÃ©penses, impressions  

#### ğŸ”¹ Couche de Traitement

- **Kafka** : Broker de messages en temps rÃ©el gÃ©rant les donnÃ©es de streaming  
- **Spark Streaming** : Traitement des Ã©vÃ©nements en temps rÃ©el depuis Kafka  
- **Spark Batch** : Traitement pÃ©riodique des donnÃ©es CRM et publicitaires  

#### ğŸ”¹ Couche de Stockage

- **MinIO** : Stockage objet servant de Data Lake  
  - **Zone Bronze** : DonnÃ©es brutes non modifiÃ©es (JSON, CSV)  
  - **Zone Silver** : DonnÃ©es nettoyÃ©es et validÃ©es  
  - **Zone Gold** : DonnÃ©es agrÃ©gÃ©es prÃªtes pour l'analyse  

#### ğŸ”¹ Data Warehouse

- **dbt** : ModÃ©lisation et transformation des donnÃ©es  
- **Snowflake** : ExÃ©cution de requÃªtes analytiques et Business Intelligence  

#### ğŸ”¹ Analytique & Visualisation

- **Metabase** : Tableaux de bord interactifs et reporting  

#### ğŸ”¹ Orchestration & Monitoring

- **Airflow** : Ordonnancement et orchestration des pipelines de donnÃ©es  
- **Prometheus/Grafana** : Surveillance des performances systÃ¨me et qualitÃ© des donnÃ©es  

Le tout est dÃ©ployÃ© dans un environnement conteneurisÃ© avec Docker pour assurer la portabilitÃ© et la reproductibilitÃ©.

### Architecture diagram

![system_architecture](https://github.com/user-attachments/assets/bd8e66f0-697e-4249-8ad4-2cf674b90779)


*Architecture diagram showing the data flow from source systems through processing to analytics*

### GÃ©nÃ©ration du diagramme d'architecture

Pour visualiser l'architecture du systÃ¨me, vous pouvez gÃ©nÃ©rer un diagramme Ã  partir du code source:

```bash
# Installez d'abord le package diagrams
pip install diagrams

# GÃ©nÃ©rez le diagramme (depuis le rÃ©pertoire racine du projet)
python docs/architecture/diagram_code.py
```

Le diagramme sera gÃ©nÃ©rÃ© dans le rÃ©pertoire `docs/architecture/` avec le nom `system_architecture.png`.

## ğŸ“‹ Structure du projet

```
project-root/
â”‚
â”œâ”€â”€ data/                      # DonnÃ©es gÃ©nÃ©rÃ©es et transformÃ©es
â”‚   â”œâ”€â”€ raw/                   # DonnÃ©es brutes
â”‚   â”‚   â”œâ”€â”€ web/               # Logs de navigation
â”‚   â”‚   â”œâ”€â”€ crm/               # DonnÃ©es clients et commandes
â”‚   â”‚   â””â”€â”€ advertising/       # DonnÃ©es des campagnes publicitaires
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/             # DonnÃ©es traitÃ©es
â”‚   â””â”€â”€ warehouse/             # DonnÃ©es finales pour analyse
â”‚
â”œâ”€â”€ src/                       # Code source
â”‚   â”œâ”€â”€ data_generation/       # Scripts de gÃ©nÃ©ration de donnÃ©es fictives
â”‚   â”‚   â”œâ”€â”€ web_data.py        # GÃ©nÃ©ration des logs web
â”‚   â”‚   â”œâ”€â”€ crm_data.py        # GÃ©nÃ©ration des donnÃ©es CRM
â”‚   â”‚   â””â”€â”€ advertising.py     # GÃ©nÃ©ration des donnÃ©es publicitaires
â”‚   â”‚
â”‚   â”œâ”€â”€ etl/                   # Pipelines d'extraction, transformation et chargement
â”‚   â”œâ”€â”€ orchestration/         # DAGs Airflow et configuration
â”‚   â””â”€â”€ dashboard/             # Configurations pour tableaux de bord
â”‚
â”œâ”€â”€ docs/                      # Documentation
â”‚   â””â”€â”€ architecture/          # Diagrammes d'architecture
â”‚       â””â”€â”€ diagram_code.py    # Code pour gÃ©nÃ©rer les diagrammes
â”‚
â”œâ”€â”€ docker/                    # Fichiers Docker pour les diffÃ©rents services
â”œâ”€â”€ docker-compose.yml         # Orchestration des conteneurs
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â””â”€â”€ README.md                  # Ce fichier
```

## ğŸš€ Installation et dÃ©marrage

### PrÃ©requis

- Docker et Docker Compose
- Python 3.8+
- Git

### Installation

1. Cloner le dÃ©pÃ´t :
   ```bash
   git clone https://github.com/votre-username/cosmetics-data-engineering.git
   cd cosmetics-data-engineering
   ```

2. Installer les dÃ©pendances :
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ“Š GÃ©nÃ©ration des donnÃ©es fictives

Le projet inclut des scripts pour gÃ©nÃ©rer des donnÃ©es fictives rÃ©alistes simulant le fonctionnement d'une entreprise de cosmÃ©tiques bio.

### GÃ©nÃ©ration des logs web

```bash
# Mode batch - GÃ©nÃ¨re des donnÃ©es historiques
python src/data_generation/web_data.py --mode batch --start-date 2025-01-01 --end-date 2025-03-28 --events-per-day 5000

# Mode streaming - GÃ©nÃ¨re des donnÃ©es en continu simulant un trafic en temps rÃ©el
python src/data_generation/web_data.py --mode stream --events-per-minute 30 --duration 3600
```

### GÃ©nÃ©ration des donnÃ©es CRM

```bash
python src/data_generation/crm_data.py --start-date 2025-01-01 --end-date 2025-03-28 --frequency daily --initial-customers 500
```

### GÃ©nÃ©ration des donnÃ©es publicitaires

```bash
python src/data_generation/advertising.py --start-date 2025-01-01 --end-date 2025-03-28 --frequency daily
```

### GÃ©nÃ©ration complÃ¨te des donnÃ©es

Pour gÃ©nÃ©rer l'ensemble des donnÃ©es pour le projet, exÃ©cutez les scripts dans cet ordre :

```bash
# 1. DonnÃ©es CRM (clients et commandes)
python src/data_generation/crm_data.py --start-date 2025-01-01 --end-date 2025-03-28

# 2. DonnÃ©es publicitaires (campagnes marketing)
python src/data_generation/advertising.py --start-date 2025-01-01 --end-date 2025-03-28

# 3. Logs web (comportement utilisateur)
python src/data_generation/web_data.py --mode batch --start-date 2025-01-01 --end-date 2025-03-28
```

Les donnÃ©es gÃ©nÃ©rÃ©es seront stockÃ©es dans le dossier `data/raw/` avec la structure suivante :
- `data/raw/crm/` : DonnÃ©es clients et commandes
- `data/raw/advertising/` : DonnÃ©es des campagnes publicitaires
- `data/raw/web/` : Logs de navigation

## ğŸ› ï¸ ExÃ©cution des composants

### 1. DÃ©marrage de l'environnement Docker

```bash
# DÃ©marrer tous les services
docker-compose up -d

# VÃ©rifier l'Ã©tat des conteneurs
docker-compose ps
```

### 2. AccÃ¨s aux interfaces utilisateur

- **Airflow**: http://localhost:8080 (utilisateur: airflow, mot de passe: airflow)
- **Kafka UI**: http://localhost:8081
- **Spark UI**: http://localhost:4040
- **Grafana**: http://localhost:3000 (utilisateur: admin, mot de passe: admin)

### 3. ExÃ©cution des pipelines de traitement

```bash
# Se connecter au conteneur Airflow
docker exec -it airflow_webserver bash

# Activer un DAG
airflow dags unpause cosmetics_daily_etl
```

### 4. Visualisation des rÃ©sultats

Une fois les pipelines exÃ©cutÃ©s, vous pouvez accÃ©der aux tableaux de bord via l'interface Tableau/Looker, oÃ¹ vous pourrez analyser:
- La performance des produits par canal
- L'efficacitÃ© des campagnes marketing
- Le comportement des utilisateurs
- L'impact des promotions sur les ventes

## ğŸ“‹ Backlog et gestion de projet

Le projet est gÃ©rÃ© en mode agile avec des sprints de 2 semaines. Le backlog du produit est organisÃ© en epics :
1. Ingestion et stockage des donnÃ©es
2. Transformation et enrichissement
3. Orchestration et automatisation
4. Dashboard - Performance produits
5. Dashboard - Performance marketing
6. Documentation et formation

Pour plus de dÃ©tails, consultez le fichier `docs/product_backlog.md`.
