# Projet Data Engineering - E-commerce de CosmÃ©tiques Biologiques

Ce projet vise Ã  amÃ©liorer le retour sur investissement (ROI) des campagnes marketing d'une entreprise de cosmÃ©tiques biologiques en fusionnant des donnÃ©es provenant de multiples sources (logs web, CRM, plateformes publicitaires) pour obtenir une vision globale des performances marketing.

## ğŸŒŸ FonctionnalitÃ©s

- Ingestion multi-source de donnÃ©es (Web, CRM, PublicitÃ©s, Influenceurs)
- Pipelines ETL pour nettoyer, transformer et enrichir les donnÃ©es
- Orchestration automatisÃ©e des flux de donnÃ©es
- Tableaux de bord interactifs pour l'analyse des performances marketing
- Environnement technique conteneurisÃ© avec Docker

## ğŸ—ï¸ Architecture du systÃ¨me

Ce projet utilise une architecture moderne de data engineering basÃ©e sur le modÃ¨le Lambda (traitement batch et temps rÃ©el) avec les composants suivants:

### ğŸ“Š Flux de DonnÃ©es

#### â­ Streaming Data Path (Logs Web)

- **Flux** : Logs Web â†’ Kafka â†’ Spark Streaming â†’ Data Lake  
- **Objectif** : OptimisÃ© pour le traitement des Ã©vÃ©nements en temps rÃ©el  
- **Avantages** : Exploite les capacitÃ©s de streaming de Kafka pour gÃ©rer des Ã©vÃ©nements Ã  haut dÃ©bit  

#### â­ Batch Data Path (CRM & PublicitÃ©s)

- **Flux** : DonnÃ©es CRM/PublicitÃ©s â†’ Spark Batch â†’ Data Lake  
- **Objectif** : Ingestion directe dans Spark pour un traitement en lot  
- **Avantages** : Ã‰vite l'utilisation inutile de Kafka pour des chargements de donnÃ©es pÃ©riodiques  

---

### ğŸ› ï¸ Composants de l'architecture

#### ğŸ”¹ Sources de donnÃ©es

- **Logs Web** : Ã‰vÃ©nements de comportement utilisateur, pages vues, conversions  
- **DonnÃ©es CRM** : Profils clients, historique des commandes  
- **DonnÃ©es Publicitaires** : Performance des campagnes, dÃ©penses, impressions  

#### ğŸ”¹ Couche de Traitement

- **Kafka** : Broker de messages en temps rÃ©el gÃ©rant les donnÃ©es de streaming  
- **Spark Streaming** : Traitement des Ã©vÃ©nements en temps rÃ©el depuis Kafka  
- **Spark Batch** : Traitement pÃ©riodique des donnÃ©es CRM et publicitaires  

#### ğŸ”¹ Couche de Stockage

- **MinIO** : Stockage objet servant de Data Lake  
  - **Zone Bronze** : DonnÃ©es brutes non modifiÃ©es (JSON, CSV)  
  - **Zone Silver** : DonnÃ©es nettoyÃ©es et validÃ©es  
  - **Zone Gold** : DonnÃ©es agrÃ©gÃ©es prÃªtes pour l'analyse  

#### ğŸ”¹ Data Warehouse

- **dbt** : ModÃ©lisation et transformation des donnÃ©es  
- **Snowflake** : ExÃ©cution de requÃªtes analytiques et Business Intelligence  

#### ğŸ”¹ Analytique & Visualisation

- **Metabase** : Tableaux de bord interactifs et reporting  

#### ğŸ”¹ Orchestration & Monitoring

- **Airflow** : Ordonnancement et orchestration des pipelines de donnÃ©es  
- **Prometheus/Grafana** : Surveillance des performances systÃ¨me et qualitÃ© des donnÃ©es  

Le tout est dÃ©ployÃ© dans un environnement conteneurisÃ© avec Docker pour assurer la portabilitÃ© et la reproductibilitÃ©.

### Architecture diagram

![system_architecture](https://github.com/user-attachments/assets/bd8e66f0-697e-4249-8ad4-2cf674b90779)


*Architecture diagram showing the data flow from source systems through processing to analytics*

### GÃ©nÃ©ration du diagramme d'architecture

Pour visualiser l'architecture du systÃ¨me, vous pouvez gÃ©nÃ©rer un diagramme Ã  partir du code source:

```bash
# Installez d'abord le package diagrams
pip install diagrams

# GÃ©nÃ©rez le diagramme (depuis le rÃ©pertoire racine du projet)
python docs/architecture/diagram_code.py
```

Le diagramme sera gÃ©nÃ©rÃ© dans le rÃ©pertoire `docs/architecture/` avec le nom `system_architecture.png`.

## ğŸ“‹ Structure du projet

```
project-root/
â”‚
â”œâ”€â”€ data/                      # DonnÃ©es gÃ©nÃ©rÃ©es et transformÃ©es
â”‚   â”œâ”€â”€ raw/                   # DonnÃ©es brutes
â”‚   â”‚   â”œâ”€â”€ web/               # Logs de navigation
â”‚   â”‚   â”œâ”€â”€ crm/               # DonnÃ©es clients et commandes
â”‚   â”‚   â””â”€â”€ advertising/       # DonnÃ©es des campagnes publicitaires
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/             # DonnÃ©es traitÃ©es
â”‚   â””â”€â”€ warehouse/             # DonnÃ©es finales pour analyse
â”‚
â”œâ”€â”€ src/                       # Code source
â”‚   â”œâ”€â”€ data_generation/       # Scripts de gÃ©nÃ©ration de donnÃ©es fictives
â”‚   â”‚   â”œâ”€â”€ web_data.py        # GÃ©nÃ©ration des logs web
â”‚   â”‚   â”œâ”€â”€ crm_data.py        # GÃ©nÃ©ration des donnÃ©es CRM
â”‚   â”‚   â””â”€â”€ advertising.py     # GÃ©nÃ©ration des donnÃ©es publicitaires
â”‚   â”‚
â”‚   â”œâ”€â”€ etl/                   # Pipelines d'extraction, transformation et chargement
â”‚   â”œâ”€â”€ orchestration/         # DAGs Airflow et configuration
â”‚   â””â”€â”€ dashboard/             # Configurations pour tableaux de bord
â”‚
â”œâ”€â”€ docs/                      # Documentation
â”‚   â””â”€â”€ architecture/          # Diagrammes d'architecture
â”‚       â””â”€â”€ diagram_code.py    # Code pour gÃ©nÃ©rer les diagrammes
â”‚
â”œâ”€â”€ docker/                    # Fichiers Docker pour les diffÃ©rents services
â”œâ”€â”€ docker-compose.yml         # Orchestration des conteneurs
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â””â”€â”€ README.md                  # Ce fichier
```

## ğŸš€ Installation et dÃ©marrage

### PrÃ©requis

- Docker et Docker Compose
- Python 3.8+
- Git

### Installation

1. Cloner le dÃ©pÃ´t :
   ```bash
   git clone https://github.com/Cassiopea2103/campaign-marketing.git
   cd campaign-marketing
   ```

2. Installer les dÃ©pendances :
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ“Š GÃ©nÃ©ration des donnÃ©es fictives

Le projet inclut des scripts pour gÃ©nÃ©rer des donnÃ©es fictives rÃ©alistes simulant le fonctionnement d'une entreprise de cosmÃ©tiques bio.

### GÃ©nÃ©ration des logs web

```bash
# Mode batch - GÃ©nÃ¨re des donnÃ©es historiques
python src/data_generation/web_data.py --mode batch --start-date 2025-01-01 --end-date 2025-03-28 --events-per-day 5000

# Mode streaming - GÃ©nÃ¨re des donnÃ©es en continu simulant un trafic en temps rÃ©el
python src/data_generation/web_data.py --mode stream --events-per-minute 30 --duration 3600
```

### GÃ©nÃ©ration des donnÃ©es CRM

```bash
python src/data_generation/crm_data.py --start-date 2025-01-01 --end-date 2025-03-28 --frequency daily --initial-customers 500
```

### GÃ©nÃ©ration des donnÃ©es publicitaires

```bash
python src/data_generation/advertising.py --start-date 2025-01-01 --end-date 2025-03-28 --frequency daily
```

### GÃ©nÃ©ration complÃ¨te des donnÃ©es

Pour gÃ©nÃ©rer l'ensemble des donnÃ©es pour le projet, exÃ©cutez les scripts dans cet ordre :

```bash
# 1. DonnÃ©es CRM (clients et commandes)
python src/data_generation/crm_data.py --start-date 2025-01-01 --end-date 2025-03-28

# 2. DonnÃ©es publicitaires (campagnes marketing)
python src/data_generation/advertising.py --start-date 2025-01-01 --end-date 2025-03-28

# 3. Logs web (comportement utilisateur)
python src/data_generation/web_data.py --mode batch --start-date 2025-01-01 --end-date 2025-03-28
```

Les donnÃ©es gÃ©nÃ©rÃ©es seront stockÃ©es dans le dossier `data/raw/` avec la structure suivante :
- `data/raw/crm/` : DonnÃ©es clients et commandes
- `data/raw/advertising/` : DonnÃ©es des campagnes publicitaires
- `data/raw/web/` : Logs de navigation

## ğŸ› ï¸ ExÃ©cution des composants

### 1. DÃ©marrage de l'environnement Docker

```bash
# DÃ©marrer tous les services
.\init.bat

# VÃ©rifier l'Ã©tat des conteneurs
docker ps
```

### 2. AccÃ¨s aux interfaces utilisateur

- **Airflow**: http://localhost:8088 (utilisateur: admin, mot de passe: admin)
- **Kafka UI**: http://localhost:9482
- **Spark UI**: http://localhost:8080
- **MinIO**: http://localhost:9001 (utilisateur: minioadmin, mot de passe: minioadmin)
- **Grafana**: http://localhost:3001 (utilisateur: admin, mot de passe: admin)
- **Metabase**: http://localhost:3000

### 3. ExÃ©cution des pipelines de traitement

```bash
# Se connecter au conteneur Airflow
docker exec -it airflow_webserver bash

# Activer un DAG
airflow dags unpause cosmetics_daily_etl
```

### 4. Visualisation des rÃ©sultats

Une fois les pipelines exÃ©cutÃ©s, vous pouvez accÃ©der aux tableaux de bord via l'interface Tableau/Looker, oÃ¹ vous pourrez analyser:
- La performance des produits par canal
- L'efficacitÃ© des campagnes marketing
- Le comportement des utilisateurs
- L'impact des promotions sur les ventes


# Pipeline d'Analyse des Campagnes Marketing

Ce rÃ©pertoire contient l'implÃ©mentation du pipeline de donnÃ©es pour l'analyse des performances des campagnes marketing pour l'entreprise e-commerce de cosmÃ©tiques biologiques.

## Vue d'ensemble

Le pipeline de donnÃ©es intÃ¨gre les logs web, les donnÃ©es CRM et les donnÃ©es publicitaires pour fournir des insights complets sur l'efficacitÃ© des campagnes marketing, le comportement client et le ROI. Le pipeline suit une architecture Lambda avec des chemins sÃ©parÃ©s pour les donnÃ©es en streaming et par lots.

## Composants du Pipeline

### Sources de donnÃ©es
- **Logs Web** : Ã‰vÃ©nements de comportement utilisateur, vues de pages, conversions
- **DonnÃ©es CRM** : Profils clients, historique des commandes
- **DonnÃ©es Publicitaires** : MÃ©triques de performance des campagnes Google Ads, rÃ©seaux sociaux et influenceurs

### Flux de donnÃ©es
1. **Couche d'Ingestion** : Les donnÃ©es brutes sont ingÃ©rÃ©es dans la couche Bronze
2. **Couche de Transformation** : Les donnÃ©es sont nettoyÃ©es et standardisÃ©es dans la couche Silver
3. **Couche d'AgrÃ©gation** : Les analyses prÃªtes Ã  l'emploi sont crÃ©Ã©es dans la couche Gold
4. **Couche EntrepÃ´t** : Les donnÃ©es sont chargÃ©es dans l'entrepÃ´t de donnÃ©es pour le reporting

### FonctionnalitÃ©s clÃ©s
- ModÃ©lisation d'attribution multi-touch
- Segmentation client RFM
- Analyse des performances marketing
- Analyse des performances produits
- Suivi du cycle de vie client

## Structure des rÃ©pertoires

```
src/
â”œâ”€â”€ ingestion/                # Scripts d'ingestion de donnÃ©es
â”‚   â”œâ”€â”€ streaming/            # Ingestion de donnÃ©es en streaming (logs web)
â”‚   â”‚   â”œâ”€â”€ ingest_web_logs.py
â”‚   â”‚   â””â”€â”€ web_logs_to_bronze.py
â”‚   â””â”€â”€ batch/                # Ingestion de donnÃ©es par lots (CRM, publicitÃ©)
â”‚       â”œâ”€â”€ crm_to_bronze.py
â”‚       â””â”€â”€ advertising_to_bronze.py
â”œâ”€â”€ transformation/           # Scripts de transformation de donnÃ©es
â”‚   â”œâ”€â”€ clean_web_logs.py
â”‚   â”œâ”€â”€ clean_crm_data.py
â”‚   â”œâ”€â”€ create_attribution_models.py
â”‚   â”œâ”€â”€ marketing_performance.py
â”‚   â””â”€â”€ customer_segments.py
â””â”€â”€ orchestration/            # DAGs Airflow
    â””â”€â”€ dags/
        â”œâ”€â”€ data_ingestion_dag.py
        â”œâ”€â”€ data_transformation_dag.py
        â”œâ”€â”€ data_aggregation_dag.py
        â”œâ”€â”€ warehouse_loading_dag.py
        â””â”€â”€ monitoring_dag.py
```

## Description des DAGs

### DAG d'Ingestion de DonnÃ©es
S'exÃ©cute quotidiennement pour ingÃ©rer des donnÃ©es brutes de diverses sources dans la couche Bronze :
- VÃ©rifie les nouveaux fichiers de logs web et les charge dans Kafka
- Traite les logs web depuis Kafka vers la couche Bronze
- Charge les donnÃ©es CRM et publicitaires directement dans la couche Bronze

### DAG de Transformation de DonnÃ©es
Transforme les donnÃ©es de la couche Bronze vers la couche Silver :
- Nettoie et standardise les logs web, les donnÃ©es CRM et les donnÃ©es publicitaires
- CrÃ©e des profils clients enrichis
- Joint les Ã©vÃ©nements web avec les donnÃ©es clients
- CrÃ©e des modÃ¨les d'attribution marketing

### DAG d'AgrÃ©gation de DonnÃ©es
CrÃ©e des donnÃ©es agrÃ©gÃ©es prÃªtes Ã  l'emploi dans la couche Gold :
- MÃ©triques de performance marketing par canal et campagne
- Analyses de performance produit
- ModÃ¨les de segmentation client
- PrÃ©visions et tendances des ventes

### DAG de Chargement de l'EntrepÃ´t
Charge les donnÃ©es de Gold vers l'entrepÃ´t de donnÃ©es :
- ExÃ©cute les modÃ¨les dbt pour l'analyse
- CrÃ©e les tables et vues finales pour le reporting
- RafraÃ®chit les tableaux de bord

### DAG de Surveillance
Surveille la santÃ© du pipeline et la qualitÃ© des donnÃ©es :
- VÃ©rifie la fraÃ®cheur des donnÃ©es
- Valide l'exhaustivitÃ© des donnÃ©es
- Surveille les mÃ©triques d'exÃ©cution du pipeline
- GÃ©nÃ¨re des rapports de qualitÃ© des donnÃ©es

## Utilisation

### PrÃ©requis
- Docker et Docker Compose
- Python 3.8+
- Apache Airflow
- Apache Spark
- Apache Kafka

### ExÃ©cution du Pipeline
1. DÃ©marrer l'environnement :
   ```
   docker-compose up -d
   ```

2. AccÃ©der Ã  Airflow pour gÃ©rer les DAGs :
   ```
   http://localhost:8088
   ```

3. Activer les DAGs dans l'ordre suivant :
   - `data_ingestion_dag`
   - `data_transformation_dag`
   - `data_aggregation_dag`
   - `warehouse_loading_dag`

## ModÃ¨les de DonnÃ©es

### Couche Bronze (DonnÃ©es Brutes)
- `web_logs` : Ã‰vÃ©nements web bruts du site e-commerce
- `customers` : DonnÃ©es brutes des profils clients
- `orders` : DonnÃ©es brutes des commandes
- `google_ads` : DonnÃ©es brutes des campagnes Google Ads
- `social_ads` : DonnÃ©es brutes des publicitÃ©s sur rÃ©seaux sociaux
- `influencer` : DonnÃ©es brutes du marketing d'influence

### Couche Silver (DonnÃ©es NettoyÃ©es)
- `web_logs` : DonnÃ©es d'Ã©vÃ©nements web nettoyÃ©es
- `web_sessions` : DonnÃ©es de sessions web agrÃ©gÃ©es
- `customers` : Profils clients nettoyÃ©s
- `orders` : DonnÃ©es de commandes nettoyÃ©es
- `order_items` : Lignes de commande dÃ©taillÃ©es
- `marketing_attribution` : DonnÃ©es d'attribution multi-touch
- `channel_performance` : Performance des canaux avec attribution

### Couche Gold (PrÃªt pour l'Analyse)
- `marketing_performance` : Performance marketing par canal, campagne et pÃ©riode
- `customer_segments` : Segmentation client pour le marketing ciblÃ©
- `rfm_segments` : Segmentation RFM (RÃ©cence, FrÃ©quence, Montant)
- `lifecycle_segments` : DonnÃ©es des Ã©tapes du cycle de vie client

## Notes d'ImplÃ©mentation

- Les logs web sont traitÃ©s via Kafka pour la gestion d'Ã©vÃ©nements en temps rÃ©el
- Les donnÃ©es CRM et publicitaires utilisent un traitement par lots direct
- Les modÃ¨les d'attribution incluent premier contact, dernier contact, linÃ©aire et dÃ©croissance temporelle
- La segmentation client inclut RFM, affinitÃ© produit et Ã©tapes du cycle de vie
- La qualitÃ© des donnÃ©es est surveillÃ©e Ã  chaque Ã©tape du pipeline

## ğŸ“‹ Backlog et gestion de projet

Le projet est gÃ©rÃ© en mode agile avec des sprints de 2 semaines. Le backlog du produit est organisÃ© en epics :
1. Ingestion et stockage des donnÃ©es
2. Transformation et enrichissement
3. Orchestration et automatisation
4. Dashboard - Performance produits
5. Dashboard - Performance marketing
6. Documentation et formation

Pour plus de dÃ©tails, consultez le fichier `docs/product_backlog.md`.
