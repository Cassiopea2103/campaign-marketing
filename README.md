# Projet Data Engineering - E-commerce de CosmÃ©tiques Biologiques

Ce projet vise Ã  amÃ©liorer le retour sur investissement (ROI) des campagnes marketing d'une entreprise de cosmÃ©tiques biologiques en fusionnant des donnÃ©es provenant de multiples sources (logs web, CRM, plateformes publicitaires) pour obtenir une vision globale des performances marketing.

## ğŸŒŸ FonctionnalitÃ©s

- Ingestion multi-source de donnÃ©es (Web, CRM, PublicitÃ©s, Influenceurs)
- Pipelines ETL pour nettoyer, transformer et enrichir les donnÃ©es
- Orchestration automatisÃ©e des flux de donnÃ©es
- Tableaux de bord interactifs pour l'analyse des performances marketing
- Environnement technique conteneurisÃ© avec Docker

## ğŸ“‹ Structure du projet

```bash
project-root/
â”‚
â”œâ”€â”€ data/                      # DonnÃ©es gÃ©nÃ©rÃ©es et transformÃ©es
â”‚   â”œâ”€â”€ raw/                   # DonnÃ©es brutes
â”‚   â”‚   â”œâ”€â”€ web/               # Logs de navigation
â”‚   â”‚   â”œâ”€â”€ crm/               # DonnÃ©es clients et commandes
â”‚   â”‚   â””â”€â”€ advertising/       # DonnÃ©es des campagnes publicitaires
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/             # DonnÃ©es traitÃ©es
â”‚   â””â”€â”€ warehouse/             # DonnÃ©es finales pour analyse
â”‚
â”œâ”€â”€ src/                       # Code source
â”‚   â”œâ”€â”€ data_generation/       # Scripts de gÃ©nÃ©ration de donnÃ©es fictives
â”‚   â”‚   â”œâ”€â”€ web_data.py        # GÃ©nÃ©ration des logs web
â”‚   â”‚   â”œâ”€â”€ crm_data.py        # GÃ©nÃ©ration des donnÃ©es CRM
â”‚   â”‚   â””â”€â”€ advertising.py     # GÃ©nÃ©ration des donnÃ©es publicitaires
â”‚   â”‚
â”‚   â”œâ”€â”€ etl/                   # Pipelines d'extraction, transformation et chargement
â”‚   â”œâ”€â”€ orchestration/         # DAGs Airflow et configuration
â”‚   â””â”€â”€ dashboard/             # Configurations pour tableaux de bord
â”‚
â”œâ”€â”€ docker/                    # Fichiers Docker pour les diffÃ©rents services
â”œâ”€â”€ docker-compose.yml         # Orchestration des conteneurs
â”œâ”€â”€ requirements.txt           # DÃ©pendances Python
â””â”€â”€ README.md                  # Ce fichier
```

## ğŸš€ Installation et dÃ©marrage

### PrÃ©requis

- Docker et Docker Compose
- Python 3.8+
- Git

### Installation

1. Cloner le dÃ©pÃ´t :
   ```bash
   git clone https://github.com/votre-username/cosmetics-data-engineering.git
   cd cosmetics-data-engineering
   ```

2. Installer les dÃ©pendances :
   ```bash
   pip install -r requirements.txt
   ```

3. Initialiser l'environnement Docker :
   ```bash
   ./init.bat
   ```

## ğŸ³ Utilisation de l'environnement Docker

### AccÃ¨s aux interfaces web

Une fois l'environnement Docker lancÃ©, vous pouvez accÃ©der aux interfaces suivantes :

- **Airflow** : [http://localhost:8088](http://localhost:8088) (identifiants : admin/admin)
- **Spark Master** : [http://localhost:8080](http://localhost:8080)
- **Kafka UI (si configurÃ©)** : [http://localhost:9090](http://localhost:9090)

### Commandes Docker utiles

```bash
# VÃ©rifier l'Ã©tat des conteneurs
docker-compose ps

# ArrÃªter l'environnement
docker-compose down

# RedÃ©marrer un service spÃ©cifique
docker-compose restart [service]

# Voir les logs d'un service
docker-compose logs [service]
```

## ğŸ“Š GÃ©nÃ©ration des donnÃ©es fictives

Le projet inclut des scripts pour gÃ©nÃ©rer des donnÃ©es fictives rÃ©alistes simulant le fonctionnement d'une entreprise de cosmÃ©tiques bio.

### GÃ©nÃ©ration des logs web

```bash
# Mode batch - GÃ©nÃ¨re des donnÃ©es historiques
python src/data_generation/web_data.py --mode batch --start-date 2025-01-01 --end-date 2025-03-28 --events-per-day 5000

# Mode streaming - GÃ©nÃ¨re des donnÃ©es en continu simulant un trafic en temps rÃ©el
python src/data_generation/web_data.py --mode stream --events-per-minute 30 --duration 3600

# Mode streaming avec envoi vers Kafka
python src/data_generation/web_data.py --mode stream --output kafka --events-per-minute 30
```

### GÃ©nÃ©ration des donnÃ©es CRM

```bash
python src/data_generation/crm_data.py --start-date 2025-01-01 --end-date 2025-03-28 --frequency daily --initial-customers 500
```

### GÃ©nÃ©ration des donnÃ©es publicitaires

```bash
python src/data_generation/advertising.py --start-date 2025-01-01 --end-date 2025-03-28 --frequency daily
```

### GÃ©nÃ©ration complÃ¨te des donnÃ©es

```bash
# 1. DonnÃ©es CRM (clients et commandes)
python src/data_generation/crm_data.py --start-date 2025-01-01 --end-date 2025-03-28

# 2. DonnÃ©es publicitaires (campagnes marketing)
python src/data_generation/advertising.py --start-date 2025-01-01 --end-date 2025-03-28

# 3. Logs web (comportement utilisateur)
python src/data_generation/web_data.py --mode batch --start-date 2025-01-01 --end-date 2025-03-28
```

Les donnÃ©es gÃ©nÃ©rÃ©es seront stockÃ©es dans le dossier `data/raw/` avec la structure suivante :

- `data/raw/crm/` : DonnÃ©es clients et commandes
- `data/raw/advertising/` : DonnÃ©es des campagnes publicitaires
- `data/raw/web/` : Logs de navigation

## ğŸ› ï¸ Architecture technique

L'architecture du projet repose sur les technologies suivantes :

- **Apache Kafka** : Pour l'ingestion de donnÃ©es en temps rÃ©el
- **Apache Spark** : Pour le traitement batch et streaming
- **Apache Airflow** : Pour l'orchestration des pipelines de donnÃ©es
- **Snowflake** : Pour le stockage et l'analyse des donnÃ©es
- **dbt** : Pour la modÃ©lisation des donnÃ©es
- **Tableau** : Pour la visualisation et les tableaux de bord

## ğŸ“‹ Backlog et gestion de projet

Le projet est gÃ©rÃ© en mode agile avec des sprints de 2 semaines. Le backlog du produit est organisÃ© en epics :

- Ingestion et stockage des donnÃ©es
- Transformation et enrichissement
- Orchestration et automatisation
- Dashboard - Performance produits
- Dashboard - Performance marketing
- Documentation et formation

ğŸ“Œ Pour plus de dÃ©tails, consultez le fichier `docs/product_backlog.md`. ğŸš€
